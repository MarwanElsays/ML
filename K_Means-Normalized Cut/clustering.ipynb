{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Latex\n",
    "import math\n",
    "import queue\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "all_data = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_1 = torch.zeros((all_data.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_1.shape[1]):\n",
    "    all_data_1[:, i] = all_data[:, i * 125 : i * 125 + 125].mean(1)\n",
    "    \n",
    "# all_data_2 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_2 = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45)\n",
    "pca.fit(training_data_2)\n",
    "training_data_2_reduced = Tensor(pca.transform(training_data_2))\n",
    "\n",
    "pca.fit(test_data_2)\n",
    "test_data_2_reduced = Tensor(pca.transform(test_data_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_similarity_graph(data,k):\n",
    "    n = data.shape[0]\n",
    "    sim_graph = torch.zeros((n,n))\n",
    "    distances = torch.cdist(data, data)\n",
    "    distances.view(-1)[::distances.size(0) + 1] = float('inf')\n",
    "    _, indices = torch.topk(distances, k, largest=False)\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n,k)\n",
    "    sim_graph[row_indices,indices] = 1\n",
    "        \n",
    "    return sim_graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    \n",
    "    return torch.exp(-gamma*torch.cdist(data,data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta,inverse_delta = a.sum(dim=1).diag(),None\n",
    "    if torch.allclose(a, a.T): inverse_delta = torch.diag(1 / delta.diag())\n",
    "    else: inverse_delta = torch.inverse(delta)\n",
    "    \n",
    "    # Replace inf and nan values with 0\n",
    "    inverse_delta.masked_fill_(torch.isnan(inverse_delta) | torch.isinf(inverse_delta), 0)\n",
    "\n",
    "    l_a = inverse_delta @ (delta - a)\n",
    "        \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "    y.masked_fill_(torch.isnan(y) | torch.isinf(y), 0)\n",
    "    \n",
    "    # Create KMeans object\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(y)\n",
    "\n",
    "    # Predict the cluster labels\n",
    "    centroids = Tensor(kmeans.cluster_centers_)\n",
    "    predicted_labels = Tensor(kmeans.labels_)\n",
    "    #means, closest_means = k_means(y, k)\n",
    "\n",
    "    return centroids,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix(actual_labels: Tensor, predicted_labels: Tensor):\n",
    "    n_samples = len(actual_labels)\n",
    "\n",
    "    n_actual = int(actual_labels.max().item()) + 1\n",
    "    n_predicted = int(predicted_labels.max().item()) + 1\n",
    "    matrix = torch.zeros(n_actual, n_predicted)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        matrix[int(actual_labels[i]), int(predicted_labels[i])] += 1\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def confusion_matrix(contingency_matrix: Tensor, n: int):\n",
    "    tp = 0.5 * (torch.sum(contingency_matrix ** 2, dim=(0, 1)) - n)\n",
    "\n",
    "    column_sum = contingency_matrix.sum(0)\n",
    "    fp = torch.sum(column_sum * (column_sum - 1) / 2) - tp\n",
    "\n",
    "    row_sum = contingency_matrix.sum(1)\n",
    "    fn = torch.sum(row_sum * (row_sum - 1) / 2) - tp\n",
    "\n",
    "    tn = n * (n - 1) / 2 - tp - fp - fn\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def precision(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, fp, _, _ = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, _, _, fn = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, fp, _, fn = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_in_partition_count = (clustering[partition_indices] == cluster_label).sum()\n",
    "            cluster_entropy -= cluster_in_partition_count / len(cluster_indices) * torch.log2(torch.Tensor([cluster_in_partition_count / len(cluster_indices)])) if cluster_in_partition_count > 0 else 0        \n",
    "        \n",
    "        total_entropy += len(cluster_indices) / len(labels) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.11590203887171845\n",
      "Recall for training set = 0.4345941551005451\n",
      "Fscore for training set = 0.18299980711736907\n",
      "Entropy for training set= 3.1263300819373834\n",
      "test:\n",
      "Precision for test set = 0.11350424351676537\n",
      "Recall for test set = 0.4331371191135734\n",
      "Fscore for test set = 0.17987259803193195\n",
      "Entropy for test set = 3.125200587369048\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.13162913490147973\n",
      "Recall for training set = 0.3876840845586551\n",
      "Fscore for training set = 0.19653079780475433\n",
      "Entropy for training set= 2.9672391610046582\n",
      "test:\n",
      "Precision for test set = 0.12769444337330815\n",
      "Recall for test set = 0.38221375807940905\n",
      "Fscore for test set = 0.19143278318929388\n",
      "Entropy for test set = 2.9673707019790183\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.13181794660531404\n",
      "Recall for training set = 0.3518799241903715\n",
      "Fscore for training set = 0.1917895110106905\n",
      "Entropy for training set= 2.8555771776464614\n",
      "test:\n",
      "Precision for test set = 0.12281447254424688\n",
      "Recall for test set = 0.34359418282548476\n",
      "Fscore for test set = 0.1809500653435857\n",
      "Entropy for test set = 2.8917619171496436\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.14420089455890944\n",
      "Recall for training set = 0.2804956254866932\n",
      "Fscore for training set = 0.1904782271853739\n",
      "Entropy for training set= 2.6292788910478735\n",
      "test:\n",
      "Precision for test set = 0.14621123680732923\n",
      "Recall for test set = 0.28477608494921514\n",
      "Fscore for test set = 0.1932189718429533\n",
      "Entropy for test set = 2.554759782040594\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.15865114224652332\n",
      "Recall for training set = 0.24833593857358802\n",
      "Fscore for training set = 0.19361194579528426\n",
      "Entropy for training set= 2.4841098770523105\n",
      "test:\n",
      "Precision for test set = 0.15121280013831756\n",
      "Recall for test set = 0.2422668513388735\n",
      "Fscore for test set = 0.18620454111979984\n",
      "Entropy for test set = 2.4695101611841532\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_1,k)\n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_1):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "    \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.1966055110115192\n",
      "Recall for k:19 = 0.25924515235457063\n",
      "Fscore for k:19 = 0.22362159256088093\n",
      "Entropy for k:19 = 2.5256583029513533\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_1, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_1,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.15102886119511214\n",
      "Recall for training set = 0.7754590662360863\n",
      "Fscore for training set = 0.2528186201017893\n",
      "Entropy for training set= 2.582197226799685\n",
      "test:\n",
      "Precision for test set = 0.13868255442071598\n",
      "Recall for test set = 0.7447368421052631\n",
      "Fscore for test set = 0.2338232736128022\n",
      "Entropy for test set = 2.7277154776910995\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.19799971078235717\n",
      "Recall for training set = 0.6046462026476112\n",
      "Fscore for training set = 0.2983127958336592\n",
      "Entropy for training set= 2.2948415933824236\n",
      "test:\n",
      "Precision for test set = 0.17428512029634669\n",
      "Recall for test set = 0.5321791320406278\n",
      "Fscore for test set = 0.2625777701846552\n",
      "Entropy for test set = 2.45794161183158\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.28666091804370375\n",
      "Recall for training set = 0.4806303249965645\n",
      "Fscore for training set = 0.35912811843710335\n",
      "Entropy for training set= 1.9580414032949707\n",
      "test:\n",
      "Precision for test set = 0.2317160955945868\n",
      "Recall for test set = 0.4458448753462604\n",
      "Fscore for test set = 0.3049450549450549\n",
      "Entropy for test set = 2.1963457931999053\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.2949203026202694\n",
      "Recall for training set = 0.3720540744812423\n",
      "Fscore for training set = 0.3290270331341744\n",
      "Entropy for training set= 1.7582819239832244\n",
      "test:\n",
      "Precision for test set = 0.24224485132394527\n",
      "Recall for test set = 0.39153970452446907\n",
      "Fscore for test set = 0.2993082637773739\n",
      "Entropy for test set = 1.9812833053815484\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.3745502369974495\n",
      "Recall for training set = 0.35263000435161007\n",
      "Fscore for training set = 0.3632597372482995\n",
      "Entropy for training set= 1.642794851337213\n",
      "test:\n",
      "Precision for test set = 0.288046016567931\n",
      "Recall for test set = 0.4057479224376731\n",
      "Fscore for test set = 0.3369129253460991\n",
      "Entropy for test set = 1.9031986971269206\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "centroids,training_predicted_labels = None,None\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_2_reduced,k)\n",
    "    \n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_2_reduced):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "        \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.34169388368238957\n",
      "Recall for k:19 = 0.4069367497691597\n",
      "Fscore for k:19 = 0.37147237163041363\n",
      "Entropy for k:19 = 1.8331897265602024\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_2_reduced, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_2_reduced,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustring(data,k):\n",
    "    num_of_clusters = len(data)\n",
    "    \n",
    "    labels = torch.arange(num_of_clusters)\n",
    "    distances = torch.cdist(data,data)\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "    \n",
    "    while(num_of_clusters > k):\n",
    "        \n",
    "        #get min distance between clusters\n",
    "        indices = torch.argmin(distances)\n",
    "        row_index, col_index = torch.unravel_index(indices, distances.shape)\n",
    "       \n",
    "        # get the elements of the two clusters obtained from step above\n",
    "        cluster1 = torch.nonzero(labels == labels[row_index]).flatten()\n",
    "        cluster2 = torch.nonzero(labels == labels[col_index]).flatten()\n",
    "\n",
    "        # Apply the mask to replace the labels to make them one cluster\n",
    "        labels[cluster1] = labels[col_index].item()\n",
    "        \n",
    "        grouped_cluster = torch.flatten(torch.cat((cluster1, cluster2)))\n",
    "        \n",
    "        # Broadcast the row indices to match the shape of the column indices\n",
    "        broadcasted_rows_indices = grouped_cluster.unsqueeze(1).expand(-1, grouped_cluster.size(0))\n",
    "\n",
    "        # Set values using broadcasted indices\n",
    "        distances[broadcasted_rows_indices, grouped_cluster] = float('inf')\n",
    "        num_of_clusters-=1\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:38 = 0.06044494288920742\n",
      "Recall for k:38 = 0.9194252077562327\n",
      "Fscore for k:38 = 0.11343258928158222\n",
      "Entropy for k:38 = 3.6770500152232457\n"
     ]
    }
   ],
   "source": [
    "k = 38\n",
    "test_predicted_labels = hierarchical_clustring(test_data_2_reduced,k)\n",
    "\n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
