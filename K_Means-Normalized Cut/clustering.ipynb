{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Latex\n",
    "import math\n",
    "import queue\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "all_data = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_1 = torch.zeros((all_data.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_1.shape[1]):\n",
    "    all_data_1[:, i] = all_data[:, i * 125 : i * 125 + 125].mean(1)\n",
    "    \n",
    "# all_data_2 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_2 = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45)\n",
    "pca.fit(training_data_2)\n",
    "training_data_2_reduced = Tensor(pca.transform(training_data_2))\n",
    "\n",
    "pca.fit(test_data_2)\n",
    "test_data_2_reduced = Tensor(pca.transform(test_data_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_similarity_graph(data,k):\n",
    "    n = data.shape[0]\n",
    "    sim_graph = torch.zeros((n,n))\n",
    "    distances = torch.cdist(data, data)\n",
    "    distances.view(-1)[::distances.size(0) + 1] = float('inf')\n",
    "    _, indices = torch.topk(distances, k, largest=False)\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n,k)\n",
    "    sim_graph[row_indices,indices] = 1\n",
    "        \n",
    "    return sim_graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    \n",
    "    return torch.exp(-gamma*torch.cdist(data,data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta = a.sum(dim=1).diag()\n",
    "    inverse_delta = torch.diag(1 / delta.diag())\n",
    "\n",
    "    # Replace inf and nan values with 0\n",
    "    inverse_delta.masked_fill_(torch.isnan(inverse_delta) | torch.isinf(inverse_delta), 0)\n",
    "\n",
    "    l_a = inverse_delta @ (delta - a)\n",
    "        \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "    y.masked_fill_(torch.isnan(y) | torch.isinf(y), 0)\n",
    "    \n",
    "    # Create KMeans object\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(y)\n",
    "\n",
    "    # Predict the cluster labels\n",
    "    centroids = Tensor(kmeans.cluster_centers_)\n",
    "    predicted_labels = Tensor(kmeans.labels_)\n",
    "    #means, closest_means = k_means(y, k)\n",
    "\n",
    "    return centroids,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_precision += len(actual_cluster_labels[actual_cluster_labels == mode])\n",
    "\n",
    "    return total_precision / len(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_recall = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_recall += len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "\n",
    "    return total_recall / len(cluster_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_f = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "\n",
    "        precision = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(cluster_indices)\n",
    "        recall = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "        total_f += 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return total_f / len(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_in_partition_count = (clustering[partition_indices] == cluster_label).sum()\n",
    "            cluster_entropy -= cluster_in_partition_count / len(cluster_indices) * torch.log2(torch.Tensor([cluster_in_partition_count / len(cluster_indices)])) if cluster_in_partition_count > 0 else 0        \n",
    "        \n",
    "        total_entropy += len(cluster_indices) / len(labels) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.23012609649122806\n",
      "Recall for training set = 0.0036804678731762065\n",
      "Fscore for training set = 0.33235224169793587\n",
      "Entropy for training set= 3.1477321000007628\n",
      "test:\n",
      "Precision for test set = 0.22149122807017543\n",
      "Recall for test set = 0.014663182346109178\n",
      "Fscore for test set = 0.32636551041255585\n",
      "Entropy for test set = 3.166743999840403\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.24246162280701755\n",
      "Recall for training set = 0.0041614912676904555\n",
      "Fscore for training set = 0.24587193125743811\n",
      "Entropy for training set= 3.0717479349558277\n",
      "test:\n",
      "Precision for test set = 0.22149122807017543\n",
      "Recall for test set = 0.01535888077858881\n",
      "Fscore for test set = 0.21714736960285735\n",
      "Entropy for test set = 3.1383983635236783\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.28686951754385964\n",
      "Recall for training set = 0.013694775963149075\n",
      "Fscore for training set = 0.26076386313952643\n",
      "Entropy for training set= 2.8221341774314\n",
      "test:\n",
      "Precision for test set = 0.2889254385964912\n",
      "Recall for test set = 0.06238162878787878\n",
      "Fscore for test set = 0.27471229493617066\n",
      "Entropy for test set = 2.817092615378948\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.3411458333333333\n",
      "Recall for training set = 0.00833132497857755\n",
      "Fscore for training set = 0.2533740047350895\n",
      "Entropy for training set= 2.609844845889453\n",
      "test:\n",
      "Precision for test set = 0.31085526315789475\n",
      "Recall for test set = 0.025791484716157206\n",
      "Fscore for test set = 0.26263056881499386\n",
      "Entropy for test set = 2.616216142236241\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.37924890350877194\n",
      "Recall for training set = 7.205729166666668\n",
      "Fscore for training set = 0.22011201966484578\n",
      "Entropy for training set= 2.447053463545908\n",
      "test:\n",
      "Precision for test set = 0.37335526315789475\n",
      "Recall for test set = 0.04729166666666667\n",
      "Fscore for test set = 0.23414199716246945\n",
      "Entropy for test set = 2.4003140233619837\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_1,k)\n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_1):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "    \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.32127192982456143\n",
      "Recall for k:19 = 0.07536008230452676\n",
      "Fscore for k:19 = 0.3161804259185218\n",
      "Entropy for k:19 = 2.590691426867207\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_1, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_1,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.31537828947368424\n",
      "Recall for training set = 0.015604654947916666\n",
      "Fscore for training set = 0.5335763981555788\n",
      "Entropy for training set= 2.4628966977251396\n",
      "test:\n",
      "Precision for test set = 0.25548245614035087\n",
      "Recall for test set = 0.050564236111111105\n",
      "Fscore for test set = 0.41208124477861324\n",
      "Entropy for test set = 2.70392040891322\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.35635964912280704\n",
      "Recall for training set = 0.017632378472222224\n",
      "Fscore for training set = 0.43072130245822415\n",
      "Entropy for training set= 2.2824933094104822\n",
      "test:\n",
      "Precision for test set = 0.3267543859649123\n",
      "Recall for test set = 0.06467013888888888\n",
      "Fscore for test set = 0.384606765084729\n",
      "Entropy for test set = 2.4709029559561007\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.42269736842105265\n",
      "Recall for training set = 0.004972910216718266\n",
      "Fscore for training set = 0.406460595120177\n",
      "Entropy for training set= 2.0654550446346844\n",
      "test:\n",
      "Precision for test set = 0.3755482456140351\n",
      "Recall for test set = 0.015998692077727954\n",
      "Fscore for test set = 0.3491360581371505\n",
      "Entropy for test set = 2.3285384351487055\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.5354989035087719\n",
      "Recall for training set = 0.04443004002911208\n",
      "Fscore for training set = 0.39223552484284696\n",
      "Entropy for training set= 1.7347384185832475\n",
      "test:\n",
      "Precision for test set = 0.4457236842105263\n",
      "Recall for test set = 0.40327380952380953\n",
      "Fscore for test set = 0.35150595146035285\n",
      "Entropy for test set = 2.0772341811253625\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.5723684210526315\n",
      "Recall for training set = 0.04588607594936709\n",
      "Fscore for training set = 0.32913530444121497\n",
      "Entropy for training set= 1.5820165170913192\n",
      "test:\n",
      "Precision for test set = 0.47423245614035087\n",
      "Recall for test set = 0.6436011904761901\n",
      "Fscore for test set = 0.2883983201965057\n",
      "Entropy for test set = 1.9512045771755406\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "centroids,training_predicted_labels = None,None\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_2_reduced,k)\n",
    "    \n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_2_reduced):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "        \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.4780701754385965\n",
      "Recall for k:19 = 0.133578431372549\n",
      "Fscore for k:19 = 0.48242725869926173\n",
      "Entropy for k:19 = 1.8723464335154814\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_2_reduced, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_2_reduced,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
