{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Latex\n",
    "import math\n",
    "import queue\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "all_data = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_1 = torch.zeros((all_data.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_1.shape[1]):\n",
    "    all_data_1[:, i] = all_data[:, i * 125 : i * 125 + 125].mean(1)\n",
    "    \n",
    "# all_data_2 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_2 = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45)\n",
    "pca.fit(training_data_2)\n",
    "training_data_2_reduced = Tensor(pca.transform(training_data_2))\n",
    "\n",
    "pca.fit(test_data_2)\n",
    "test_data_2_reduced = Tensor(pca.transform(test_data_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_similarity_graph(data,k):\n",
    "    n = data.shape[0]\n",
    "    sim_graph = torch.zeros((n,n))\n",
    "    distances = torch.cdist(data, data)\n",
    "    distances.view(-1)[::distances.size(0) + 1] = float('inf')\n",
    "    _, indices = torch.topk(distances, k, largest=False)\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n,k)\n",
    "    sim_graph[row_indices,indices] = 1\n",
    "        \n",
    "    return sim_graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    \n",
    "    return torch.exp(-gamma*torch.cdist(data,data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta,inverse_delta = a.sum(dim=1).diag(),None\n",
    "    if torch.allclose(a, a.T): inverse_delta = torch.diag(1 / delta.diag())\n",
    "    else: inverse_delta = torch.inverse(delta)\n",
    "    \n",
    "    # Replace inf and nan values with 0\n",
    "    inverse_delta.masked_fill_(torch.isnan(inverse_delta) | torch.isinf(inverse_delta), 0)\n",
    "\n",
    "    l_a = inverse_delta @ (delta - a)\n",
    "        \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "    y.masked_fill_(torch.isnan(y) | torch.isinf(y), 0)\n",
    "    \n",
    "    # Create KMeans object\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(y)\n",
    "\n",
    "    # Predict the cluster labels\n",
    "    centroids = Tensor(kmeans.cluster_centers_)\n",
    "    predicted_labels = Tensor(kmeans.labels_)\n",
    "    #means, closest_means = k_means(y, k)\n",
    "\n",
    "    return centroids,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_precision += len(actual_cluster_labels[actual_cluster_labels == mode])\n",
    "\n",
    "    return total_precision / len(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_recall = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_recall += len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "\n",
    "    return total_recall / len(cluster_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_f = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "\n",
    "        precision = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(cluster_indices)\n",
    "        recall = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "        total_f += 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return total_f / len(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_in_partition_count = (clustering[partition_indices] == cluster_label).sum()\n",
    "            cluster_entropy -= cluster_in_partition_count / len(cluster_indices) * torch.log2(torch.Tensor([cluster_in_partition_count / len(cluster_indices)])) if cluster_in_partition_count > 0 else 0        \n",
    "        \n",
    "        total_entropy += len(cluster_indices) / len(labels) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.23190789473684212\n",
      "Recall for training set = 0.005767342931937172\n",
      "Fscore for training set = 0.3394410352006251\n",
      "Entropy for training set= 3.0965992377006213\n",
      "test:\n",
      "Precision for test set = 0.22697368421052633\n",
      "Recall for test set = 0.022578534031413612\n",
      "Fscore for test set = 0.32736256130280805\n",
      "Entropy for test set = 3.1312049795234516\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.25753837719298245\n",
      "Recall for training set = 0.0042183010057471266\n",
      "Fscore for training set = 0.29284372088512023\n",
      "Entropy for training set= 3.005280127878413\n",
      "test:\n",
      "Precision for test set = 0.23684210526315788\n",
      "Recall for test set = 0.015410958904109588\n",
      "Fscore for test set = 0.2811016156479973\n",
      "Entropy for test set = 3.0685019996568537\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.3085252192982456\n",
      "Recall for training set = 0.02307865813648294\n",
      "Fscore for training set = 0.29709706121779933\n",
      "Entropy for training set= 2.73643299821176\n",
      "test:\n",
      "Precision for test set = 0.29660087719298245\n",
      "Recall for test set = 0.07224893162393162\n",
      "Fscore for test set = 0.2782935193248532\n",
      "Entropy for test set = 2.767486917607124\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.34539473684210525\n",
      "Recall for training set = 0.12867647058823525\n",
      "Fscore for training set = 0.25975475593791486\n",
      "Entropy for training set= 2.5451689422702803\n",
      "test:\n",
      "Precision for test set = 0.3399122807017544\n",
      "Recall for test set = 0.5381944444444445\n",
      "Fscore for test set = 0.28161686981215694\n",
      "Entropy for test set = 2.5208253507913567\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.3826754385964912\n",
      "Recall for training set = 0.09825450450450453\n",
      "Fscore for training set = 0.23570938975471897\n",
      "Entropy for training set= 2.4443438220203104\n",
      "test:\n",
      "Precision for test set = 0.37225877192982454\n",
      "Recall for test set = 0.7858796296296295\n",
      "Fscore for test set = 0.2425205210042836\n",
      "Entropy for test set = 2.3833002724095866\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_1,k)\n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_1):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "    \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.31359649122807015\n",
      "Recall for k:19 = 0.12159863945578232\n",
      "Fscore for k:19 = 0.3075610158840629\n",
      "Entropy for k:19 = 2.555096522921013\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_1, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_1,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.2578125\n",
      "Recall for training set = 0.04755764563106795\n",
      "Fscore for training set = 0.4318531509227939\n",
      "Entropy for training set= 2.6933293558126903\n",
      "test:\n",
      "Precision for test set = 0.25164473684210525\n",
      "Recall for test set = 0.19921875\n",
      "Fscore for test set = 0.41445588485883683\n",
      "Entropy for test set = 2.720140266344455\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.37184758771929827\n",
      "Recall for training set = 0.009785462834718374\n",
      "Fscore for training set = 0.4386563757933679\n",
      "Entropy for training set= 2.3228831638384646\n",
      "test:\n",
      "Precision for test set = 0.3125\n",
      "Recall for test set = 0.04039115646258503\n",
      "Fscore for test set = 0.4031678621782181\n",
      "Entropy for test set = 2.5331109178264515\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.41639254385964913\n",
      "Recall for training set = 0.37673611111111116\n",
      "Fscore for training set = 0.3851845085873407\n",
      "Entropy for training set= 2.1239828316688305\n",
      "test:\n",
      "Precision for test set = 0.37335526315789475\n",
      "Recall for test set = 0.028149801587301588\n",
      "Fscore for test set = 0.364606346477959\n",
      "Entropy for test set = 2.3137063428787803\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.5176809210526315\n",
      "Recall for training set = 0.030930621069182394\n",
      "Fscore for training set = 0.40395629791059867\n",
      "Entropy for training set= 1.750432500703608\n",
      "test:\n",
      "Precision for test set = 0.46271929824561403\n",
      "Recall for test set = 0.10466269841269843\n",
      "Fscore for test set = 0.3653098882889586\n",
      "Entropy for test set = 1.9419894451912323\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.528234649122807\n",
      "Recall for training set = 0.10454644097222221\n",
      "Fscore for training set = 0.32774751997145285\n",
      "Entropy for training set= 1.6639299904486313\n",
      "test:\n",
      "Precision for test set = 0.524671052631579\n",
      "Recall for test set = 0.8307291666666666\n",
      "Fscore for test set = 0.35431124358886906\n",
      "Entropy for test set = 1.7624221824009463\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "centroids,training_predicted_labels = None,None\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_2_reduced,k)\n",
    "    \n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_2_reduced):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "        \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.48464912280701755\n",
      "Recall for k:19 = 0.07426075268817203\n",
      "Fscore for k:19 = 0.4916830377834458\n",
      "Entropy for k:19 = 1.873102622060991\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_2_reduced, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_2_reduced,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustring(data,k):\n",
    "    num_of_clusters = len(data)\n",
    "    \n",
    "    labels = torch.arange(num_of_clusters)\n",
    "    distances = torch.cdist(data,data)\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "    \n",
    "    while(num_of_clusters > k):\n",
    "        \n",
    "        #get min distance between clusters\n",
    "        indices = torch.argmin(distances)\n",
    "        row_index, col_index = torch.unravel_index(indices, distances.shape)\n",
    "       \n",
    "        # get the elements of the two clusters obtained from step above\n",
    "        cluster1 = torch.nonzero(labels == labels[row_index]).flatten()\n",
    "        cluster2 = torch.nonzero(labels == labels[col_index]).flatten()\n",
    "        \n",
    "        mask = cluster1 if labels[row_index] > labels[col_index] else cluster2\n",
    "        \n",
    "        # Apply the mask to replace the labels to make them one cluster\n",
    "        min_val = min(labels[row_index].item(),labels[col_index].item())\n",
    "        labels[mask] = min_val\n",
    "        \n",
    "        grouped_cluster = torch.flatten(torch.cat((cluster1, cluster2)))\n",
    "        \n",
    "        # Broadcast the row indices to match the shape of the column indices\n",
    "        broadcasted_rows_indices = grouped_cluster.unsqueeze(1).expand(-1, grouped_cluster.size(0))\n",
    "\n",
    "        # Set values using broadcasted indices\n",
    "        distances[broadcasted_rows_indices, grouped_cluster] = float('inf')\n",
    "        num_of_clusters-=1\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.12828947368421054\n",
      "Recall for k:19 = 2.437499999999999\n",
      "Fscore for k:19 = 0.10222675915547319\n",
      "Entropy for k:19 = 3.850657794069117\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "test_predicted_labels = hierarchical_clustring(test_data_2_reduced,k)\n",
    "\n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
