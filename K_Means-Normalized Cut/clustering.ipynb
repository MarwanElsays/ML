{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Latex\n",
    "import math\n",
    "import queue\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "all_data = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_1 = torch.zeros((all_data.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_1.shape[1]):\n",
    "    all_data_1[:, i] = all_data[:, i * 125 : i * 125 + 125].mean(1)\n",
    "    \n",
    "# all_data_2 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_2 = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45)\n",
    "pca.fit(training_data_2)\n",
    "training_data_2_reduced = Tensor(pca.transform(training_data_2))\n",
    "\n",
    "pca.fit(test_data_2)\n",
    "test_data_2_reduced = Tensor(pca.transform(test_data_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_similarity_graph(data,k):\n",
    "    n = data.shape[0]\n",
    "    sim_graph = torch.zeros((n,n))\n",
    "    distances = torch.cdist(data, data)\n",
    "    distances.view(-1)[::distances.size(0) + 1] = float('inf')\n",
    "    _, indices = torch.topk(distances, k, largest=False)\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n,k)\n",
    "    sim_graph[row_indices,indices] = 1\n",
    "        \n",
    "    return sim_graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    \n",
    "    return torch.exp(-gamma*torch.cdist(data,data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta,inverse_delta = a.sum(dim=1).diag(),None\n",
    "    if torch.allclose(a, a.T): inverse_delta = torch.diag(1 / delta.diag())\n",
    "    else: inverse_delta = torch.inverse(delta)\n",
    "    \n",
    "    # Replace inf and nan values with 0\n",
    "    inverse_delta.masked_fill_(torch.isnan(inverse_delta) | torch.isinf(inverse_delta), 0)\n",
    "\n",
    "    l_a = inverse_delta @ (delta - a)\n",
    "        \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "    y.masked_fill_(torch.isnan(y) | torch.isinf(y), 0)\n",
    "    \n",
    "    # Create KMeans object\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(y)\n",
    "\n",
    "    # Predict the cluster labels\n",
    "    centroids = Tensor(kmeans.cluster_centers_)\n",
    "    predicted_labels = Tensor(kmeans.labels_)\n",
    "    #means, closest_means = k_means(y, k)\n",
    "\n",
    "    return centroids,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_precision += len(actual_cluster_labels[actual_cluster_labels == mode])\n",
    "\n",
    "    return total_precision / len(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_recall = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_recall += len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "\n",
    "    return total_recall / len(cluster_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_f = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "\n",
    "        precision = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(cluster_indices)\n",
    "        recall = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "        total_f += 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return total_f / len(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_in_partition_count = (clustering[partition_indices] == cluster_label).sum()\n",
    "            cluster_entropy -= cluster_in_partition_count / len(cluster_indices) * torch.log2(torch.Tensor([cluster_in_partition_count / len(cluster_indices)])) if cluster_in_partition_count > 0 else 0        \n",
    "        \n",
    "        total_entropy += len(cluster_indices) / len(labels) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.23807565789473684\n",
      "Recall for training set = 0.0019215962192013598\n",
      "Fscore for training set = 0.35126422778561733\n",
      "Entropy for training set= 3.104138121877752\n",
      "test:\n",
      "Precision for test set = 0.22532894736842105\n",
      "Recall for test set = 0.0070299671592775025\n",
      "Fscore for test set = 0.3298140206034293\n",
      "Entropy for test set = 3.1341217244694386\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.26726973684210525\n",
      "Recall for training set = 0.002291572653429603\n",
      "Fscore for training set = 0.3060329147307327\n",
      "Entropy for training set= 2.971093349919154\n",
      "test:\n",
      "Precision for test set = 0.2450657894736842\n",
      "Recall for test set = 0.008000429553264604\n",
      "Fscore for test set = 0.2682592938963677\n",
      "Entropy for test set = 3.0169876034164202\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.3086622807017544\n",
      "Recall for training set = 0.011107165404040402\n",
      "Fscore for training set = 0.2981587577802812\n",
      "Entropy for training set= 2.7266966617727424\n",
      "test:\n",
      "Precision for test set = 0.2889254385964912\n",
      "Recall for test set = 0.04990530303030303\n",
      "Fscore for test set = 0.27737126301325005\n",
      "Entropy for test set = 2.7399633754247215\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.31811951754385964\n",
      "Recall for training set = 0.10421156609195405\n",
      "Fscore for training set = 0.24222052212584036\n",
      "Entropy for training set= 2.69638569556029\n",
      "test:\n",
      "Precision for test set = 0.31030701754385964\n",
      "Recall for test set = 0.26799242424242425\n",
      "Fscore for test set = 0.2433006497716303\n",
      "Entropy for test set = 2.664547721437284\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.3684210526315789\n",
      "Recall for training set = 0.10769230769230768\n",
      "Fscore for training set = 0.21826180958856833\n",
      "Entropy for training set= 2.512128353042546\n",
      "test:\n",
      "Precision for test set = 0.3514254385964912\n",
      "Recall for test set = 0.3927696078431373\n",
      "Fscore for test set = 0.23799040180188963\n",
      "Entropy for test set = 2.5176674270480177\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_1,k)\n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_1):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "    \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.3207236842105263\n",
      "Recall for k:19 = 0.21763392857142855\n",
      "Fscore for k:19 = 0.32190980710345857\n",
      "Entropy for k:19 = 2.562849849415997\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_1, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_1,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.3227796052631579\n",
      "Recall for training set = 0.007344685628742514\n",
      "Fscore for training set = 0.5360964053411444\n",
      "Entropy for training set= 2.5123140153569206\n",
      "test:\n",
      "Precision for test set = 0.26535087719298245\n",
      "Recall for test set = 0.018467643467643464\n",
      "Fscore for test set = 0.41620025870617305\n",
      "Entropy for test set = 2.6971131922950002\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.41310307017543857\n",
      "Recall for training set = 0.03519712257100149\n",
      "Fscore for training set = 0.5123882323624032\n",
      "Entropy for training set= 2.1396168999287823\n",
      "test:\n",
      "Precision for test set = 0.35526315789473684\n",
      "Recall for test set = 0.18750000000000003\n",
      "Fscore for test set = 0.4549987535967672\n",
      "Entropy for test set = 2.338355839571734\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.48026315789473684\n",
      "Recall for training set = 0.022201946472019462\n",
      "Fscore for training set = 0.48166826800414214\n",
      "Entropy for training set= 1.939218390778826\n",
      "test:\n",
      "Precision for test set = 0.4166666666666667\n",
      "Recall for test set = 0.060432569974554706\n",
      "Fscore for test set = 0.40457419058541727\n",
      "Entropy for test set = 2.1130507505060034\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.5287828947368421\n",
      "Recall for training set = 0.10357603092783506\n",
      "Fscore for training set = 0.40115017083643384\n",
      "Entropy for training set= 1.732120827421565\n",
      "test:\n",
      "Precision for test set = 0.44298245614035087\n",
      "Recall for test set = 0.3659420289855073\n",
      "Fscore for test set = 0.3400641865489144\n",
      "Entropy for test set = 2.025588357766282\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.5370065789473685\n",
      "Recall for training set = 0.37789351851851843\n",
      "Fscore for training set = 0.32836694214093276\n",
      "Entropy for training set= 1.650221310900662\n",
      "test:\n",
      "Precision for test set = 0.4692982456140351\n",
      "Recall for test set = 0.12384259259259259\n",
      "Fscore for test set = 0.3361743153183188\n",
      "Entropy for test set = 1.8700032484032527\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "centroids,training_predicted_labels = None,None\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_2_reduced,k)\n",
    "    \n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_2_reduced):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "        \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.48464912280701755\n",
      "Recall for k:19 = 0.08854166666666667\n",
      "Fscore for k:19 = 0.4877830392952398\n",
      "Entropy for k:19 = 1.848521138994532\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_2_reduced, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_2_reduced,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
