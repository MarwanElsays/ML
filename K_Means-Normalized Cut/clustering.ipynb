{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "import numpy as np\n",
    "from IPython.display import Latex\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn import cluster, metrics\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  ..., 18., 18., 18.])\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"./data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_1 = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_2 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_2 = torch.zeros((all_data_1.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_2.shape[1]):\n",
    "    all_data_2[:, i] = all_data_1[:, i * 125 : i * 125 + 125].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9120, 5625])\n"
     ]
    }
   ],
   "source": [
    "all_data_tst = torch.zeros((19 * 60 * 8, 45 * 125))\n",
    "labels = torch.zeros(19 * 60 * 8)\n",
    "\n",
    "top_folder = \"./data\"\n",
    "i = 0\n",
    "curr_label = 0\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    if len(dirs) != 0 or root == top_folder:\n",
    "        continue\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        person_data = torch.tensor(df.values)\n",
    "\n",
    "        std = person_data.std(0)\n",
    "        mean = person_data.mean(0)\n",
    "\n",
    "        person_data = (person_data - mean) / std\n",
    "        all_data_tst[i] = person_data.reshape(-1)\n",
    "\n",
    "        labels[i] = curr_label\n",
    "\n",
    "        i += 1\n",
    "        if i % 480 == 0:\n",
    "            curr_label += 1\n",
    "\n",
    "all_data_2 = torch.zeros((all_data_1.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_2.shape[1]):\n",
    "    all_data_2[:, i] = all_data_1[:, i * 125 : i * 125 + 125].mean(1)\n",
    "\n",
    "print(all_data_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data taking the even indexed rows for testing and the odd indexed rows for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data_1)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data_1)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_2(points:Tensor,k):\n",
    "    \n",
    "    '''\n",
    "        points: dataset\n",
    "        k: number of clusters\n",
    "    '''\n",
    "    plen = len(points[0]) if points.ndim > 1 else 1\n",
    "    means = points[torch.randperm(len(points))[:k]]\n",
    "    itr = 0\n",
    "    while True:\n",
    "        itr+=1\n",
    "        classes = [[] for _ in range(k)]\n",
    "        \n",
    "        for j,point in enumerate(points):\n",
    "            min_distance = torch.inf\n",
    "            nearest_mean = None\n",
    "        \n",
    "            for i in range (len(means)):\n",
    "                distance = torch.linalg.norm(point - means[i])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    nearest_mean = i\n",
    "            \n",
    "            classes[nearest_mean].append((j+1,point))\n",
    "           \n",
    "        new_means = torch.tensor([[0.0 for _ in range(plen)] for _ in range(k)]) \n",
    "        for i,c in enumerate(classes):\n",
    "            n = float(len(c))\n",
    "            new_mean = torch.tensor([0.0 for _ in range(plen)])\n",
    "            for j,point in c:\n",
    "                new_mean+=point\n",
    "            new_mean = new_mean/n\n",
    "            new_means[i] = new_mean\n",
    "        \n",
    "        if(torch.equal(new_means,means)):\n",
    "            return classes\n",
    "        \n",
    "        means = new_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019232697088924697\n",
      "0.2188774010944789\n",
      "0.021014126725182675\n",
      "0.28606526300205676\n",
      "0.029080841511425717\n",
      "0.27479506114141755\n",
      "0.01549427600185919\n",
      "0.2700093309245396\n",
      "0.020992712446163568\n",
      "0.24000115165769687\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_data_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f1_score(Tensor(mylabels), labels))\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# classes = k_means_2(all_data_1,k)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# for i in range(len(classes)):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m#print(f'accuracy for k = {k} is {accuracy}')\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m centroids, mylabels, inertia \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mk_means(\u001b[43mall_data_2\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;241m19\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m mylabels:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(l)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_data_2' is not defined"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "\n",
    "a = torch.cdist(all_data_tst, all_data_tst)\n",
    "for k in ks:\n",
    "    # means, mylabels = k_means(all_data_2, k)\n",
    "    centroids, mylabels, inertia = cluster.k_means(all_data_tst.numpy(), k)\n",
    "    # mylabels = cluster.k_means(a, k)\n",
    "    print(metrics.f1_score(labels, mylabels, average='macro'))\n",
    "    print(f1_score(Tensor(mylabels), labels))\n",
    "    \n",
    "        # classes = k_means_2(all_data_1,k)\n",
    "        \n",
    "        # for i in range(len(classes)):\n",
    "        #     classes[i] = [classes[i][j][0] for j in range(len(classes[i]))]\n",
    "        \n",
    "        # mylabels = np.zeros(19)\n",
    "        # for i,c in  enumerate(classes):\n",
    "        #     for idx in c:\n",
    "        #         mylabels[idx-1] = i\n",
    "            \n",
    "        #accuracy = torch.sum(labels == mylabels)/len(labels)\n",
    "        \n",
    "        #print(f'accuracy for k = {k} is {accuracy}')\n",
    "\n",
    "centroids, mylabels, inertia = cluster.k_means(all_data_2.numpy(), 19)\n",
    "\n",
    "for l in mylabels:\n",
    "    print(l)\n",
    "\n",
    "print(Tensor(mylabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_precision += len(actual_cluster_labels[actual_cluster_labels == mode])\n",
    "\n",
    "    return total_precision / len(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_recall = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_recall += len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "\n",
    "    return total_recall / len(cluster_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_f = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "\n",
    "        precision = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(cluster_indices)\n",
    "        recall = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "        total_f += 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return total_f / len(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_entropy -= len(partition_indices) / len(cluster_indices) * torch.log2(torch.Tensor(len(partition_indices) / len(cluster_indices)))\n",
    "        \n",
    "        total_entropy += 1 / len(cluster_indices) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2832752154123034"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Tensor(mylabels), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    graph = torch.zeros((len(data), len(data)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            graph[i][j] = torch.exp(-gamma * torch.norm(data[i] - data[j]) ** 2)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta = a.sum(dim=1).diag()\n",
    "    l_a = (1 / delta).diag().diag() @ (delta - a)\n",
    "    \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "\n",
    "    means, closest_means = k_means(y, k)\n",
    "\n",
    "    return closest_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# a = rbf_graph(all_data_2, 0.1)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcdist(\u001b[43mall_data_2\u001b[49m, all_data_2)\n\u001b[0;32m      4\u001b[0m delta \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdiag()\n\u001b[0;32m      5\u001b[0m w \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m delta)\u001b[38;5;241m.\u001b[39mdiag()\u001b[38;5;241m.\u001b[39mdiag() \u001b[38;5;241m@\u001b[39m a\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_data_2' is not defined"
     ]
    }
   ],
   "source": [
    "# a = rbf_graph(all_data_2, 0.1)\n",
    "\n",
    "a = torch.cdist(all_data_2, all_data_2)\n",
    "delta = a.sum(dim=1).diag()\n",
    "w = (1 / delta).diag().diag() @ a\n",
    "\n",
    "# mylabels = k_ways_normalised_cut(a, 19)\n",
    "# f1_score(mylabels, labels)\n",
    "\n",
    "# metrics.f1_score(labels, mylabels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)a\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, mylabels = power_method(w, 19)\n",
    "f1_score(mylabels, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
