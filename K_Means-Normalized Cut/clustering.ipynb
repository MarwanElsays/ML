{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Latex\n",
    "import math\n",
    "import queue\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data Matrix and the Label vector\n",
    "Read data into torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Specify the top-level folder\n",
    "top_folder = \"data\"\n",
    "\n",
    "# Initialize an empty list to store flattened arrays\n",
    "flattened_arrays = []\n",
    "labels = torch.zeros(9120)\n",
    "example_cnt, example_label = 0, 0\n",
    "\n",
    "for root, dirs, files in os.walk(top_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            lines = []\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split(\",\")\n",
    "                    lines.append([float(value) for value in values])\n",
    "\n",
    "            flattened_array = torch.tensor(lines).view(-1)\n",
    "            labels[example_cnt] = example_label\n",
    "\n",
    "            flattened_arrays.append(flattened_array)\n",
    "            example_cnt += 1\n",
    "\n",
    "            if example_cnt % 480 == 0:\n",
    "                example_label += 1\n",
    "\n",
    "all_data = torch.stack(flattened_arrays)\n",
    "\n",
    "# all_data_1 is a 2D tensor of shape (9120, 45) containing the mean of each column in each segment resulting in 45 features for each data point\n",
    "all_data_1 = torch.zeros((all_data.shape[0], 45))\n",
    "\n",
    "for i in range(all_data_1.shape[1]):\n",
    "    all_data_1[:, i] = all_data[:, i * 125 : i * 125 + 125].mean(1)\n",
    "    \n",
    "# all_data_2 is a 2D tensor of shape (9120, 5625) containing 45 x 125 features for each data point\n",
    "all_data_2 = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Test sets\n",
    "Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [i for i in range(len(all_data)) if (i % 60) < 48]\n",
    "test_indices = [i for i in range(len(all_data)) if (i % 60) >= 48]\n",
    "\n",
    "training_data_1 = all_data_1[training_indices]\n",
    "test_data_1 = all_data_1[test_indices]\n",
    "\n",
    "training_data_2 = all_data_2[training_indices]\n",
    "test_data_2 = all_data_2[test_indices]\n",
    "\n",
    "training_labels = labels[training_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=45)\n",
    "pca.fit(training_data_2)\n",
    "training_data_2_reduced = Tensor(pca.transform(training_data_2))\n",
    "\n",
    "pca.fit(test_data_2)\n",
    "test_data_2_reduced = Tensor(pca.transform(test_data_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(points: Tensor, k: int, relative_error: float = 1e-6, max_iterations: int = 1000):\n",
    "    n = len(points)\n",
    "    means = points[torch.randperm(n)[:k]]\n",
    "\n",
    "    error = 1\n",
    "    iterations = 0\n",
    "    while error > relative_error and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        distances = torch.cdist(points, means)\n",
    "        closest_means = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_means = torch.zeros_like(means)\n",
    "        for i in range(k):\n",
    "            new_means[i] = points[closest_means == i].mean(dim=0)\n",
    "\n",
    "        error = torch.norm(means - new_means) / torch.norm(new_means)\n",
    "\n",
    "        means = new_means\n",
    "\n",
    "    return means, closest_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Ways normalised Cut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_similarity_graph(data,k):\n",
    "    n = data.shape[0]\n",
    "    sim_graph = torch.zeros((n,n))\n",
    "    distances = torch.cdist(data, data)\n",
    "    distances.view(-1)[::distances.size(0) + 1] = float('inf')\n",
    "    _, indices = torch.topk(distances, k, largest=False)\n",
    "    row_indices = torch.arange(n).unsqueeze(1).expand(n,k)\n",
    "    sim_graph[row_indices,indices] = 1\n",
    "        \n",
    "    return sim_graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_graph(data: Tensor, gamma: float):\n",
    "    \n",
    "    return torch.exp(-gamma*torch.cdist(data,data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_ways_normalised_cut(a: Tensor, k: int):\n",
    "    delta,inverse_delta = a.sum(dim=1).diag(),None\n",
    "    if torch.allclose(a, a.T): inverse_delta = torch.diag(1 / delta.diag())\n",
    "    else: inverse_delta = torch.inverse(delta)\n",
    "    \n",
    "    # Replace inf and nan values with 0\n",
    "    inverse_delta.masked_fill_(torch.isnan(inverse_delta) | torch.isinf(inverse_delta), 0)\n",
    "\n",
    "    l_a = inverse_delta @ (delta - a)\n",
    "        \n",
    "    eigen_values, eigen_vectors = torch.linalg.eig(l_a)\n",
    "    eigen_values = eigen_values.real\n",
    "    eigen_vectors = eigen_vectors.real\n",
    "\n",
    "    indices = torch.argsort(eigen_values)\n",
    "    eigen_values = eigen_values[indices]\n",
    "    eigen_vectors = eigen_vectors[:, indices]\n",
    "\n",
    "    u = eigen_vectors[:, :k]\n",
    "    y = u / torch.norm(u, dim=1, keepdim=True)\n",
    "    y.masked_fill_(torch.isnan(y) | torch.isinf(y), 0)\n",
    "    \n",
    "    # Create KMeans object\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(y)\n",
    "\n",
    "    # Predict the cluster labels\n",
    "    centroids = Tensor(kmeans.cluster_centers_)\n",
    "    predicted_labels = Tensor(kmeans.labels_)\n",
    "    #means, closest_means = k_means(y, k)\n",
    "\n",
    "    return centroids,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix(actual_labels: Tensor, predicted_labels: Tensor):\n",
    "    n_samples = len(actual_labels)\n",
    "\n",
    "    n_actual = int(actual_labels.max().item()) + 1\n",
    "    n_predicted = int(predicted_labels.max().item()) + 1\n",
    "    matrix = torch.zeros(n_actual, n_predicted)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        matrix[int(actual_labels[i]), int(predicted_labels[i])] += 1\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def confusion_matrix(contingency_matrix: Tensor, n: int):\n",
    "    tp = 0.5 * (torch.sum(contingency_matrix ** 2, dim=(0, 1)) - n)\n",
    "\n",
    "    column_sum = contingency_matrix.sum(0)\n",
    "    fp = torch.sum(column_sum * (column_sum - 1) / 2) - tp\n",
    "\n",
    "    row_sum = contingency_matrix.sum(1)\n",
    "    fn = torch.sum(row_sum * (row_sum - 1) / 2) - tp\n",
    "\n",
    "    tn = n * (n - 1) / 2 - tp - fp - fn\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def precision_2(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, fp, _, _ = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def precision(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "        total_precision += len(actual_cluster_labels[actual_cluster_labels == mode])\n",
    "\n",
    "    return total_precision / len(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, _, _, fn = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_2(predicted_labels: Tensor, actual_labels: Tensor):\n",
    "    contingency = contingency_matrix(actual_labels, predicted_labels)\n",
    "    tp, fp, _, fn = confusion_matrix(contingency, len(actual_labels))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def f1_score(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "\n",
    "    total_f = 0.0\n",
    "\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        actual_cluster_labels = labels[cluster_indices]\n",
    "        mode = actual_cluster_labels.mode(dim=0)[0]\n",
    "\n",
    "        precision = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(cluster_indices)\n",
    "        recall = len(actual_cluster_labels[actual_cluster_labels == mode]) / len(labels[labels == mode])\n",
    "        total_f += 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return total_f / len(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(clustering: Tensor, labels: Tensor):\n",
    "    cluster_labels = torch.unique(clustering)\n",
    "    partition_labels = torch.unique(labels)\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for cluster_label in cluster_labels:\n",
    "        cluster_entropy = 0.0\n",
    "        cluster_indices = (clustering == cluster_label).nonzero()\n",
    "\n",
    "        for partition_label in partition_labels:\n",
    "            partition_indices = (labels == partition_label).nonzero()\n",
    "            cluster_in_partition_count = (clustering[partition_indices] == cluster_label).sum()\n",
    "            cluster_entropy -= cluster_in_partition_count / len(cluster_indices) * torch.log2(torch.Tensor([cluster_in_partition_count / len(cluster_indices)])) if cluster_in_partition_count > 0 else 0        \n",
    "        \n",
    "        total_entropy += len(cluster_indices) / len(labels) * cluster_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution1:Taking the mean of each column in each segment for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.22930372807017543\n",
      "Recall for training set = 0.4881848060098026\n",
      "Fscore for training set = 0.1934170586164415\n",
      "Entropy for training set= 3.150761364192166\n",
      "test:\n",
      "Precision for test set = 0.22861842105263158\n",
      "Recall for test set = 0.4821445060018467\n",
      "Fscore for test set = 0.1892518812832141\n",
      "Entropy for test set = 3.1473447107732375\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.2735745614035088\n",
      "Recall for training set = 0.45972470340341715\n",
      "Fscore for training set = 0.1929711768982569\n",
      "Entropy for training set= 2.9612778801742037\n",
      "test:\n",
      "Precision for test set = 0.2631578947368421\n",
      "Recall for test set = 0.45090027700831026\n",
      "Fscore for test set = 0.18049885992695217\n",
      "Entropy for test set = 2.994771398422983\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.2974232456140351\n",
      "Recall for training set = 0.33198061243186294\n",
      "Fscore for training set = 0.18951288596348762\n",
      "Entropy for training set= 2.8459469469925933\n",
      "test:\n",
      "Precision for test set = 0.27960526315789475\n",
      "Recall for test set = 0.3311634349030471\n",
      "Fscore for test set = 0.18461182102459173\n",
      "Entropy for test set = 2.8373268477357594\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.31661184210526316\n",
      "Recall for training set = 0.2578250251935321\n",
      "Fscore for training set = 0.1856180759519761\n",
      "Entropy for training set= 2.6638988491583566\n",
      "test:\n",
      "Precision for test set = 0.31469298245614036\n",
      "Recall for test set = 0.26188827331486614\n",
      "Fscore for test set = 0.18738902423917084\n",
      "Entropy for test set = 2.5900846965996314\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.39418859649122806\n",
      "Recall for training set = 0.2284566671247309\n",
      "Fscore for training set = 0.20093379991854274\n",
      "Entropy for training set= 2.3857082459831958\n",
      "test:\n",
      "Precision for test set = 0.3843201754385965\n",
      "Recall for test set = 0.2315904893813481\n",
      "Fscore for test set = 0.20035347685425572\n",
      "Entropy for test set = 2.3665942213263893\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_1,k)\n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_1):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "    \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.19043922807205374\n",
      "Recall for k:19 = 0.24636426592797783\n",
      "Fscore for k:19 = 0.21482163614679728\n",
      "Entropy for k:19 = 2.547638670779158\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.1,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_1, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_1,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution2:Flattening all the features together for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ For k = 8 ------\n",
      "training:\n",
      "Precision for training set = 0.32140899122807015\n",
      "Recall for training set = 0.7234308437542943\n",
      "Fscore for training set = 0.24722184039224185\n",
      "Entropy for training set= 2.5221248885709384\n",
      "test:\n",
      "Precision for test set = 0.27028508771929827\n",
      "Recall for test set = 0.685387811634349\n",
      "Fscore for test set = 0.22514587733034058\n",
      "Entropy for test set = 2.6703336474780945\n",
      "------ For k = 13 ------\n",
      "training:\n",
      "Precision for training set = 0.38061951754385964\n",
      "Recall for training set = 0.5910767658375704\n",
      "Fscore for training set = 0.2683288237831322\n",
      "Entropy for training set= 2.290392241607976\n",
      "test:\n",
      "Precision for test set = 0.33497807017543857\n",
      "Recall for test set = 0.5798822714681441\n",
      "Fscore for test set = 0.23940930361108204\n",
      "Entropy for test set = 2.4266484048341783\n",
      "------ For k = 19 ------\n",
      "training:\n",
      "Precision for training set = 0.49917763157894735\n",
      "Recall for training set = 0.4785633102010902\n",
      "Fscore for training set = 0.3710778996591631\n",
      "Entropy for training set= 1.8843416426164439\n",
      "test:\n",
      "Precision for test set = 0.4479166666666667\n",
      "Recall for test set = 0.4583333333333333\n",
      "Fscore for test set = 0.3272703297427825\n",
      "Entropy for test set = 2.091418050904501\n",
      "------ For k = 28 ------\n",
      "training:\n",
      "Precision for training set = 0.5209703947368421\n",
      "Recall for training set = 0.4046625211854702\n",
      "Fscore for training set = 0.3608960164634376\n",
      "Entropy for training set= 1.7475111806102102\n",
      "test:\n",
      "Precision for test set = 0.45997807017543857\n",
      "Recall for test set = 0.4007733148661127\n",
      "Fscore for test set = 0.3116644152533625\n",
      "Entropy for test set = 2.0018586138195995\n",
      "------ For k = 38 ------\n",
      "training:\n",
      "Precision for training set = 0.5705866228070176\n",
      "Recall for training set = 0.3553776739498878\n",
      "Fscore for training set = 0.37633433885612944\n",
      "Entropy for training set= 1.5566305500433548\n",
      "test:\n",
      "Precision for test set = 0.5054824561403509\n",
      "Recall for test set = 0.34344413665743306\n",
      "Fscore for test set = 0.3190341914248035\n",
      "Entropy for test set = 1.7697984020572328\n"
     ]
    }
   ],
   "source": [
    "ks = [8, 13, 19, 28,38]\n",
    "centroids,training_predicted_labels = None,None\n",
    "for k in ks:\n",
    "    centroids,training_predicted_labels = k_means(training_data_2_reduced,k)\n",
    "    \n",
    "    test_predicted_labels = torch.empty_like(test_labels, dtype=torch.long)\n",
    "    for i,point in enumerate(test_data_2_reduced):\n",
    "        distances = torch.norm(point - centroids, dim=1)\n",
    "        test_predicted_labels[i] = torch.argmin(distances)\n",
    "        \n",
    "    prec_train     = precision(training_predicted_labels,training_labels)\n",
    "    rec_train      = recall(training_predicted_labels,training_labels)\n",
    "    f_score_train  = f1_score(training_predicted_labels,training_labels)\n",
    "    entropy_train  = conditional_entropy(training_predicted_labels,training_labels)\n",
    "    \n",
    "    prec_test    = precision(test_predicted_labels,test_labels)\n",
    "    rec_test     = recall(test_predicted_labels,test_labels)\n",
    "    f_score_test = f1_score(test_predicted_labels,test_labels)\n",
    "    entropy_test  = conditional_entropy(test_predicted_labels,test_labels)\n",
    "    \n",
    "    print(f'------ For k = {k} ------')\n",
    "    print(\"training:\")\n",
    "    print(f'Precision for training set = {prec_train}')\n",
    "    print(f'Recall for training set = {rec_train}')\n",
    "    print(f'Fscore for training set = {f_score_train}')  \n",
    "    print(f'Entropy for training set= {entropy_train.item()}')     \n",
    "    print(\"test:\")\n",
    "    print(f'Precision for test set = {prec_test}')\n",
    "    print(f'Recall for test set = {rec_test}')\n",
    "    print(f'Fscore for test set = {f_score_test}')  \n",
    "    print(f'Entropy for test set = {entropy_test.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Normalized Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.5060307017543859\n",
      "Recall for k:19 = 0.4137119113573407\n",
      "Fscore for k:19 = 0.36683126519124987\n",
      "Entropy for k:19 = 1.844753891319193\n"
     ]
    }
   ],
   "source": [
    "alpha,k = 0.001,19\n",
    "\n",
    "#sim_graph = rbf_graph(test_data_2_reduced, alpha)\n",
    "sim_graph = KNN_similarity_graph(test_data_2_reduced,100)\n",
    "centroids,test_predicted_labels = k_ways_normalised_cut(sim_graph,k)\n",
    "    \n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustring(data,k):\n",
    "    num_of_clusters = len(data)\n",
    "    \n",
    "    labels = torch.arange(num_of_clusters)\n",
    "    distances = torch.cdist(data,data)\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "    \n",
    "    while(num_of_clusters > k):\n",
    "        \n",
    "        #get min distance between clusters\n",
    "        indices = torch.argmin(distances)\n",
    "        row_index, col_index = torch.unravel_index(indices, distances.shape)\n",
    "       \n",
    "        # get the elements of the two clusters obtained from step above\n",
    "        cluster1 = torch.nonzero(labels == labels[row_index]).flatten()\n",
    "        cluster2 = torch.nonzero(labels == labels[col_index]).flatten()\n",
    "\n",
    "        # Apply the mask to replace the labels to make them one cluster\n",
    "        labels[cluster1] = labels[col_index].item()\n",
    "        \n",
    "        grouped_cluster = torch.flatten(torch.cat((cluster1, cluster2)))\n",
    "        \n",
    "        # Broadcast the row indices to match the shape of the column indices\n",
    "        broadcasted_rows_indices = grouped_cluster.unsqueeze(1).expand(-1, grouped_cluster.size(0))\n",
    "\n",
    "        # Set values using broadcasted indices\n",
    "        distances[broadcasted_rows_indices, grouped_cluster] = float('inf')\n",
    "        num_of_clusters-=1\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for k:19 = 0.12828947368421054\n",
      "Recall for k:19 = 0.959960757156048\n",
      "Fscore for k:19 = 0.11003390808171278\n",
      "Entropy for k:19 = 3.850657794069117\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "test_predicted_labels = hierarchical_clustring(test_data_2_reduced,k)\n",
    "\n",
    "prec    = precision(test_predicted_labels,test_labels)\n",
    "rec     = recall(test_predicted_labels,test_labels)\n",
    "f_score = f1_score(test_predicted_labels,test_labels)\n",
    "entropy = conditional_entropy(test_predicted_labels,test_labels)\n",
    "\n",
    "print(f'Precision for k:{k} = {prec}')\n",
    "print(f'Recall for k:{k} = {rec}')\n",
    "print(f'Fscore for k:{k} = {f_score}')  \n",
    "print(f'Entropy for k:{k} = {entropy.item()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(a: Tensor, k: int):\n",
    "    n = len(a)\n",
    "    v = torch.randn(n)\n",
    "    v /= torch.norm(v)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        v = a @ v\n",
    "        v /= torch.norm(v)\n",
    "\n",
    "    return k_means(v[:, None], k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
